{% load static %}

<div class="body-container">
    <div class="ui centered one column grid container">
        <!--<div class="ui vertical buttons row">
            <center>
                <button id="recordButton" class="ui red inverted big button record-button" role="switch">
                    Mic &nbsp;&nbsp;<i class="microphone icon"></i>
                </button>
            </center>
        </div>-->

        <canvas
            id="axesDiv"
            class="ui centered"
            style="width: 800px; height: 388px;"
        ></canvas>
    </div>
</div>

<script>
    exports = {};
</script>
<script>
    // From a series of URL to js files, get an object URL that can be loaded in an
    // AudioWorklet. This is useful to be able to use multiple files (utils, data
    // structure, main DSP, etc.) without either using static imports, eval, manual
    // concatenation with or without a build step, etc.
    function URLFromFiles(files) {
        const promises = files
            .map((file) => fetch(file)
            .then((response) => response.text()));

        return Promise
            .all(promises)
            .then((texts) => {
            texts.unshift("var exports = {};"); // hack to make injected umd modules work
            const text = texts.join('');
            const blob = new Blob([text], {type: "application/javascript"});

            return URL.createObjectURL(blob);
            });
    }
</script>
<script>
(function() {
    let AudioContext;
    // global var for web audio API AudioContext
    let audioCtx;
    let bufferSize = 8192;

    try {
        AudioContext = window.AudioContext || window.webkitAudioContext;
        audioCtx = new AudioContext();
    } catch (e) {
        throw "Could not instantiate AudioContext: " + e.message;
    }

    // global var getUserMedia mic stream
    let gumStream;
    // global audio node variables
    let mic;
    let gain;
    let pitchNode;

    // Shared data with AudioWorkletGlobalScope
    let audioReader;

    // Visualization objects
    let animationId;
    let canvas = document.getElementById("axesDiv");
    let pitchAccum = [];
    let rmsAccum = [];
    let refreshRate = bufferSize / audioCtx.sampleRate * 1000;

    // calculate time axis labels
    function getTimeLabels(n) {
        // where `n` is number of pitch values or time frames to be represented
        let xlabels = [];
        for (let i = 0; i < n; i++) {
            xlabels.push(Math.round(Math.round(i * refreshRate) / 100) / 10) // time in secs rounded to 1 decimal place
        }
        return xlabels;
    }

    DATA.labels = getTimeLabels(30);

    // console.log("before chart creation");
    // console.log(RMS_ARRAY);
    // console.log(CONFIDENCE_ARRAY);
    let pitchChart = new Chart(canvas.getContext("2d"), {
        "data": DATA,
        "options": OPTIONS
    });
    // console.log("after chart creation");

    // Utils:
    function arraySum(total, num) {
        return total + num;
    }

    function resetChartData() {
        rms_pointer = RMS_ARRAY;
        pitchChart.data.labels = getTimeLabels(30);
        pitchChart.data.datasets[0].data = Array(30).fill(100);
        pitchChart.data.datasets[1].data = Array(30).fill(AXES_PITCHES[0]);
        pitchChart.data.datasets[2].data = Array(30).fill(AXES_PITCHES.slice(-1)[0]);
        pitchChart.update();
    }

    function onRecordClickHandler() {
        let recording = $(this).hasClass("recording");
        if (!recording) {
            $(this).prop("disabled", true);

            resetChartData();
            // start microphone stream using getUserMedia and runs the feature extraction
            startMicRecordStream();
        } else {
            stopMicRecordStream();
        }
    }

    // record native microphone input and do further audio processing on each audio buffer using the given callback functions
    function startMicRecordStream() {
        if (navigator.mediaDevices.getUserMedia) {
            console.log("Initializing audio...");
            navigator.mediaDevices.getUserMedia({ audio: true, video: false })
            .then(startAudioProcessing)
            .catch(function(message) {
                    throw "Could not access microphone - " + message;
            });
        } else {
            throw "Could not access microphone - getUserMedia not available";
        }
    }

    function startAudioProcessing(stream) {
        gumStream = stream;
        if (gumStream.active) {
            if (audioCtx.state == "closed") {
                audioCtx = new AudioContext();
            }
            else if (audioCtx.state == "suspended") {
                audioCtx.resume();
            }

            mic = audioCtx.createMediaStreamSource(gumStream);
            gain = audioCtx.createGain();
            gain.gain.setValueAtTime(0, audioCtx.currentTime);

            let codeForProcessorModule = ["https://cdn.jsdelivr.net/npm/essentia.js@0.1.3/dist/essentia-wasm.umd.js",
                                        "https://cdn.jsdelivr.net/npm/essentia.js@0.1.3/dist/essentia.js-core.umd.js",
                                        "pitchyinprob-processor.js",
                                        "https://unpkg.com/ringbuf.js@0.1.0/dist/index.js"];

            // inject Essentia.js code into AudioWorkletGlobalScope context, then setup audio graph and start animation
            URLFromFiles(codeForProcessorModule)
            .then((concatenatedCode) => {
                audioCtx.audioWorklet.addModule(concatenatedCode)
                .then(setupAudioGraph)
                .catch( function moduleLoadRejected(msg) {
                    console.log(`There was a problem loading the AudioWorklet module code: \n ${msg}`);
                });
            })
            .catch((msg) => {
                console.log(`There was a problem retrieving the AudioWorklet module code: \n ${msg}`);
            })

            // set button to stop
            $("#recordButton").addClass("recording");
            $("#recordButton").html('Stop &nbsp;&nbsp;<i class="stop icon"></i>');
            $("#recordButton").prop("disabled", false);
        } else {
            throw "Mic stream not active";
        }
    }

    function setupAudioGraph() {
        let sab = exports.RingBuffer.getStorageForCapacity(3, Float32Array); // capacity: three float32 values [pitch, confidence, rms]
        let rb = new exports.RingBuffer(sab, Float32Array);
        audioReader = new exports.AudioReader(rb);

        pitchNode = new AudioWorkletNode(audioCtx, 'pitchyinprob-processor', {
            processorOptions: {
                bufferSize: bufferSize,
                sampleRate: audioCtx.sampleRate,
            }
        });

        try {
            pitchNode.port.postMessage({
                sab: sab,
            });
        } catch(_){
            alert("No SharedArrayBuffer tranfer support, try another browser.");
            $("#recordButton").off('click', onRecordClickHandler);
            $("#recordButton").prop("disabled", true);
            return;
        }

        // It seems necessary to connect the stream to a sink for the pipeline to work, contrary to documentataions.
        // As a workaround, here we create a gain node with zero gain, and connect temp to the system audio output.
        mic.connect(pitchNode);
        pitchNode.connect(gain);
        gain.connect(audioCtx.destination);

        requestAnimationFrame(animatePitch); // start plot animation
    }

    let animationStart = 0;
    let elapsed;
    // draw melspectrogram frames
    function animatePitch(timestamp) {
        // if (animationStart === undefined)

        animationId = requestAnimationFrame(animatePitch);
        /* SAB method */
        let pitchBuffer = new Float32Array(3);
        if (audioReader.available_read() >= 1) {
            let read = audioReader.dequeue(pitchBuffer);
            if (read !== 0) {
                // console.info("main: ", pitchBuffer[0], pitchBuffer[1], pitchBuffer[2]);
                // elapsed = timestamp - animationStart;
                // animationStart = timestamp;
                // console.info(elapsed);
                CONFIDENCE_ARRAY.push(pitchBuffer[1]);
                CONFIDENCE_ARRAY.shift();
                const logRMS = 1 + Math.log10(pitchBuffer[2] + Number.MIN_VALUE) * 0.5;
                rmsAccum.push(logRMS);
                RMS_ARRAY.push(logRMS);
                RMS_ARRAY.shift();
                pitchAccum.push(pitchBuffer[0]);
                pitchChart.data.datasets[0].data.push(pitchBuffer[0]);
                pitchChart.data.datasets[0].data.shift();

                // console.info("before chart update");
                pitchChart.update();
                // console.info("AFTER chart update");
            }
        }
    }

    function drawFullPitchContour() {
        rms_pointer = rmsAccum;
        pitchChart.data.datasets[0].data = pitchAccum;
        pitchChart.data.datasets[1].data = Array(pitchAccum.length).fill(AXES_PITCHES[0]);
        pitchChart.data.datasets[2].data = Array(pitchAccum.length).fill(AXES_PITCHES.slice(-1)[0]);
        pitchChart.data.labels = getTimeLabels(pitchAccum.length);
        pitchChart.update();
        pitchAccum = [];
        rmsAccum = [];
        console.info("Full pitch contour should be displaying");
    }

    function stopMicRecordStream() {
        if (animationId) {
            cancelAnimationFrame(animationId);
            drawFullPitchContour();
        }

        // stop mic stream
        gumStream.getAudioTracks().forEach(function(track) {
            track.stop();
            gumStream.removeTrack(track);
        });

        audioCtx.close().then(function() {
            // manage button state
            $("#recordButton").removeClass("recording");
            $("#recordButton").html('Mic &nbsp;&nbsp;<i class="microphone icon"></i>');

            // disconnect nodes
            mic.disconnect();
            pitchNode.disconnect();
            gain.disconnect();
            mic = undefined;
            pitchNode = undefined;
            gain = undefined;

            console.log("Stopped recording ...");
        });
    }

    $(document).ready(function() {
        // check for SharedArrayBuffer support:
        // add event listeners to ui objects
        $("#recordButton").on('click', onRecordClickHandler);


        try {
            const testSAB = new SharedArrayBuffer(1);
            delete testSAB;
        } catch (e) {
            if (e instanceof ReferenceError && !crossOriginIsolated) {
                $("#recordButton").prop("disabled", true);
                // redirect to cross-origin isolated SAB-capable version on Netlify
                //window.location = "https://essentiajs-pitchmelodia.netlify.app";
                return;
            }

            // Unknown malfunction: alert user and offer alternative
            $("#recordButtonContainer").before(`
                <div class="ui message">
                    <div class="header">Unable to run app</div>
                    <p><a href="https://essentiajs-melspectrogram.netlify.app">Check out this version! <i class="external alternate icon"></i><a/></p>
                    <p style="font-weight: 300;"><a href="https://github.com/MTG/essentia.js/issues">Let us know <i class="icon comment"></i></a></p>
                </div>`);
        }
    });
})();
</script>
